{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Running ToG on myself dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please retrieve 3 relations (separated by semicolon) that contribute to the question and rate their contribution on a scale from 0 to 1 (the sum of the scores of 3 relations is 1).\n",
      "Q: Mesih Pasha's uncle became emperor in what year?\n",
      "Topic Entity: Mesih Pasha\n",
      "Relations:\n",
      "1. wiki.relation.child\n",
      "2. wiki.relation.country_of_citizenship\n",
      "3. wiki.relation.date_of_birth\n",
      "4. wiki.relation.family\n",
      "5. wiki.relation.father\n",
      "6. wiki.relation.languages_spoken, written_or_signed\n",
      "7. wiki.relation.military_rank\n",
      "8. wiki.relation.occupation\n",
      "9. wiki.relation.place_of_death\n",
      "10. wiki.relation.position_held\n",
      "11. wiki.relation.religion_or_worldview\n",
      "12. wiki.relation.sex_or_gender\n",
      "13. wiki.relation.sibling\n",
      "14. wiki.relation.significant_event\n",
      "A: 1. {wiki.relation.family (Score: 0.5)}: This relation is highly relevant as it can provide information about the family background of Mesih Pasha, including his uncle who became emperor.\n",
      "2. {wiki.relation.father (Score: 0.4)}: Uncle is father's brother, so father might provide some information as well.\n",
      "3. {wiki.relation.position held (Score: 0.1)}: This relation is moderately relevant as it can provide information about any significant positions held by Mesih Pasha or his uncle that could be related to becoming an emperor.\n",
      "\n",
      "Q: Van Andel Institute was founded in part by what American businessman, who was best known as co-founder of the Amway Corporation?\n",
      "Topic Entity: Van Andel Institute\n",
      "Relations:\n",
      "1. wiki.relation.affiliation\n",
      "2. wiki.relation.country\n",
      "3. wiki.relation.donations\n",
      "4. wiki.relation.educated_at\n",
      "5. wiki.relation.employer\n",
      "6. wiki.relation.headquarters_location\n",
      "7. wiki.relation.legal_form\n",
      "8. wiki.relation.located_in_the_administrative_territorial_entity\n",
      "9. wiki.relation.total_revenue\n",
      "A: 1. {wiki.relation.affiliation (Score: 0.4)}: This relation is relevant because it can provide information about the individuals or organizations associated with the Van Andel Institute, including the American businessman who co-founded the Amway Corporation.\n",
      "2. {wiki.relation.donations (Score: 0.3)}: This relation is relevant because it can provide information about the financial contributions made to the Van Andel Institute, which may include donations from the American businessman in question.\n",
      "3. {wiki.relation.educated_at (Score: 0.3)}: This relation is relevant because it can provide information about the educational background of the American businessman, which may have influenced his involvement in founding the Van Andel Institute.\n",
      "\n",
      "Q: Ai là phó giám đốc khối của phòng của Vũ Hải Yến?\n",
      "Topic Entity: Vũ Hải Yến\n",
      "Relations:\n",
      "1. Trung tâm sản phẩm Conversation have relations: Bà Vũ Hải Yến làm việc tại Trung tâm sản phẩm Conversation.A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. {wiki.relation.affiliation (Score: 0.5)}: This relation is highly relevant as it can provide information about the organization or department where Vũ Hải Yến works, which may help identify the deputy director of her division.\n",
      "2. {wiki.relation.position_held (Score: 0.4)}: This relation is relevant because it can provide insights into the specific roles and titles held by Vũ Hải Yến, potentially leading to information about her deputy director.\n",
      "3. {wiki.relation.employer (Score: 0.1)}: This relation is moderately relevant as it can give context about the organization Vũ Hải Yến is associated with, but may not directly lead to identifying the deputy director.\n",
      "[{'entity': 'Vũ Hải Yến', 'relation': 'affiliation', 'score': 0.5, 'head': False}, {'entity': 'Vũ Hải Yến', 'relation': 'position held', 'score': 0.4, 'head': False}, {'entity': 'Vũ Hải Yến', 'relation': 'employer', 'score': 0.1, 'head': False}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "stop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23824/2605539042.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_entity_relations_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"stop\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mException\u001b[0m: stop"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import random\n",
    "from test_func import *\n",
    "from client import *\n",
    "from utils import *\n",
    "\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--dataset\", type=str,\n",
    "#                     default=\"webqsp\", help=\"choose the dataset.\")\n",
    "# parser.add_argument(\"--max_length\", type=int,\n",
    "#                     default=256, help=\"the max length of LLMs output.\")\n",
    "# parser.add_argument(\"--temperature_exploration\", type=float,\n",
    "#                     default=0.4, help=\"the temperature in exploration stage.\")\n",
    "# parser.add_argument(\"--temperature_reasoning\", type=float,\n",
    "#                     default=0, help=\"the temperature in reasoning stage.\")\n",
    "# parser.add_argument(\"--width\", type=int,\n",
    "#                     default=3, help=\"choose the search width of ToG.\")\n",
    "# parser.add_argument(\"--depth\", type=int,\n",
    "#                     default=3, help=\"choose the search depth of ToG.\")\n",
    "# parser.add_argument(\"--remove_unnecessary_rel\", type=bool,\n",
    "#                     default=True, help=\"whether removing unnecessary relations.\")\n",
    "# parser.add_argument(\"--LLM_type\", type=str,\n",
    "#                     default=\"gpt-3.5-turbo\", help=\"base LLM model.\")\n",
    "# parser.add_argument(\"--opeani_api_keys\", type=str,\n",
    "#                     default=\"\", help=\"if the LLM_type is gpt-3.5-turbo or gpt-4, you need add your own openai api keys.\")\n",
    "# parser.add_argument(\"--num_retain_entity\", type=int,\n",
    "#                     default=5, help=\"Number of entities retained during entities search.\")\n",
    "# parser.add_argument(\"--prune_tools\", type=str,\n",
    "#                     default=\"llm\", help=\"prune tools for ToG, can be llm (same as LLM_type), bm25 or sentencebert.\")\n",
    "# parser.add_argument(\"--addr_list\", type=str,\n",
    "#                     default=\"server_urls.txt\", help=\"The address of the Wikidata service.\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args_dict = {\n",
    "    \"dataset\": \"myself\",\n",
    "    \"max_length\": 256,\n",
    "    \"temperature_exploration\": 0.4,\n",
    "    \"temperature_reasoning\": 0,\n",
    "    \"width\": 3,\n",
    "    \"depth\": 3,\n",
    "    \"remove_unnecessary_rel\": True,\n",
    "    \"LLM_type\": \"gpt-4o-mini\",\n",
    "    \"opeani_api_keys\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "    \"num_retain_entity\": 5,\n",
    "    \"prune_tools\": \"llm\",\n",
    "    \"addr_list\": \"F:\\\\CodingEnvironment\\\\tog\\\\ToG\\\\server_urls.txt\"\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**args_dict)\n",
    "\n",
    "    \n",
    "datas, question_string = prepare_dataset(args.dataset)\n",
    "print(\"Start Running ToG on %s dataset.\" % args.dataset)\n",
    "for data in tqdm(datas):\n",
    "    question = data[question_string]\n",
    "    topic_entity = data['qid_topic_entity']\n",
    "    cluster_chain_of_entities = []\n",
    "    if len(topic_entity) == 0:\n",
    "        results = generate_without_explored_paths(question, args)\n",
    "        save_2_jsonl(question, results, [], file_name=args.dataset)\n",
    "        continue\n",
    "    pre_relations = []\n",
    "    pre_heads= [-1] * len(topic_entity)\n",
    "    flag_printed = False\n",
    "\n",
    "    # with open(args.addr_list, \"r\") as f:\n",
    "    #     server_addrs = f.readlines()\n",
    "    #     server_addrs = [addr.strip() for addr in server_addrs]\n",
    "    # print(f\"Server addresses: {server_addrs}\")\n",
    "    # wiki_client = MultiServerWikidataQueryClient(server_addrs)\n",
    "\n",
    "    import json\n",
    "\n",
    "    file_path = r'F:\\CodingEnvironment\\tog\\fci_graph.json'\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        graph = json.load(file)\n",
    "\n",
    "    for depth in range(1, args.depth+1):\n",
    "        current_entity_relations_list = []\n",
    "        i=0\n",
    "        for entity in topic_entity:\n",
    "            if entity!=\"[FINISH_ID]\":\n",
    "                retrieve_relations_with_scores = relation_search_prune(entity, pre_relations, pre_heads[i], question, args, graph)  # best entity triplet, entitiy_id\n",
    "                current_entity_relations_list.extend(retrieve_relations_with_scores)\n",
    "            i+=1\n",
    "        total_candidates = []\n",
    "        total_scores = []\n",
    "        total_relations = []\n",
    "        total_entities_id = []\n",
    "        total_topic_entities = []\n",
    "        total_head = []\n",
    "\n",
    "        print(current_entity_relations_list)\n",
    "\n",
    "        raise Exception(\"stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth 1 still not find the answer.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "stop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23824/3226818486.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m# topic_entity = {qid: topic for qid, topic in zip(entities_id, [wiki_client.query_all(\"qid2label\", entity).pop() for entity in entities_id])}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# continue #@\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"stop\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mhalf_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster_chain_of_entities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: stop"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import random\n",
    "from test_func import *\n",
    "from client import *\n",
    "from utils import *\n",
    "\n",
    "#duyet lai ten entity\n",
    "for entity in current_entity_relations_list:\n",
    "    value_flag=False\n",
    "    \n",
    "    entity_candidates = entity_search(entity['entity'], entity['relation'], graph, entity['head'])\n",
    "\n",
    "    scores, entity_candidates = entity_score(question, entity_candidates, entity['score'], entity['relation'], args)\n",
    "    \n",
    "    total_candidates, total_scores, total_relations, total_topic_entities, total_head = update_history(entity_candidates, entity, scores, total_candidates, total_scores, total_relations, total_topic_entities, total_head, value_flag)\n",
    "\n",
    "# raise Exception(\"stop\")\n",
    "\n",
    "if len(total_candidates) ==0:\n",
    "    half_stop(question, cluster_chain_of_entities, depth, args)\n",
    "    flag_printed = True\n",
    "    # break #@\n",
    "    raise Exception(\"stop\")\n",
    "    \n",
    "flag, chain_of_entities, pre_relations, pre_heads = entity_prune( total_relations, total_candidates, total_topic_entities, total_head, total_scores, args, graph)\n",
    "cluster_chain_of_entities.append(chain_of_entities)\n",
    "if flag:\n",
    "    stop, results = reasoning(question, cluster_chain_of_entities, args)\n",
    "    if stop:\n",
    "        print(\"ToG stoped at depth %d.\" % depth)\n",
    "        save_2_jsonl(question, results, cluster_chain_of_entities, file_name=args.dataset)\n",
    "        flag_printed = True\n",
    "        # break #@\n",
    "        raise Exception(\"stop\")\n",
    "    else:\n",
    "        print(\"depth %d still not find the answer.\" % depth)\n",
    "        # flag_finish, entities_id = if_finish_list(entities_id)\n",
    "        # if flag_finish:\n",
    "        #     half_stop(question, cluster_chain_of_entities, depth, args)\n",
    "        #     flag_printed = True\n",
    "        # else:\n",
    "\n",
    "        # topic_entity = {qid: topic for qid, topic in zip(entities_id, [wiki_client.query_all(\"qid2label\", entity).pop() for entity in entities_id])}\n",
    "        # continue #@\n",
    "        raise Exception(\"stop\")\n",
    "else:\n",
    "    half_stop(question, cluster_chain_of_entities, depth, args)\n",
    "    flag_printed = True\n",
    "\n",
    "raise Exception(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[('affiliation', 'Trung tâm sản phẩm Conversation'), ('position held', 'Trung tâm sản phẩm Conversation'), ('employer', 'Trung tâm sản phẩm Conversation')]]]\n"
     ]
    }
   ],
   "source": [
    "print(cluster_chain_of_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trung tâm sản phẩm Conversation']\n"
     ]
    }
   ],
   "source": [
    "print(entity_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vũ Hải Yến', 'Vũ Hải Yến', 'Vũ Hải Yến']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_topic_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
